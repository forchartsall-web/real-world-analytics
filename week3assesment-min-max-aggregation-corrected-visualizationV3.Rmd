---
title: "R Notebook"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you
execute code within the notebook, the results appear beneath the code.

Try executing this chunk by clicking the *Run* button within the chunk
or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*.

```{r}
the.data <- as.matrix(read.table("ENB.txt"))  
```

```{r}
set.seed(226156731)
```

```{r}
num_row<-650
```

# Loading and Inspecting the Dataset

#### Justification: Understanding structure and ranges of variables is essential before transformation; summary statistics reveal potential skewness or ##outliers.

```{r}
install.packages("tidyverse") # if needed
install.packages("car") 
library(tidyverse)
library(tidyverse)
library(car)
library(corrplot)
library(GGally)

# Load your file
df <- as.matrix(read.table("mydata2.txt", header = TRUE))

# Inspect structure
str(df)
summary(df)


```

install.packages("tidyverse")

# Visualising Distributions of Variables

```{r Histogram + Density for Each Variable}
df %>% 
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 30, alpha = 0.5) +
  geom_density() +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of All Variables")


```

#### Interpretation / Critical Comment:

-   Most X variables appear approximately uniform or moderately skewed.

-   Y shows a multi-modal distribution, suggesting potential grouping or
    latent clusters.

-   Since values are already scaled (0–1), further standardization is
    not necessary, but transformations may be appropriate depending on
    skewnes

# Correlation Analysis

## Correlation Matrix & Heatmap

```{r Correlation Matrix & Heatmap}
install.packages("corrplot")
library(corrplot)

cor_matrix <- cor(df)

corrplot(cor_matrix, method = "color", type = "upper",
         tl.col = "black", tl.cex = 0.7)

```

#### Critical Interpretation:

-   Strong relationships between X1-X3 and X2-X4
-   Medium relationships between X2-X5 and X4-X5
-   Weak correlations between predictors and **Y** may suggest the need
    for polynomial terms, interaction effects, nonlinear models.\

# Scatterplots to Examine Predictor–Response Relationships

```{r Scatterplots to Examine Predictor–Response Relationships}
df %>% 
  pivot_longer(cols = starts_with("X"), 
               names_to = "predictor", values_to = "value") %>%
  ggplot(aes(value, Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ predictor, scales = "free_x") +
  theme_minimal() +
  labs(title = "Relationships Between Predictors and Y")

```

#### Critical Interpretation:

-   Some predictors show nonlinear relationships (bow-shaped, plateau).

-   Linear modelling may be insufficient; transformations can capture
    nonlinearity.

AM

```{r REUSLT, paged.print=TRUE}
par(mfrow=c(2,3))
```

```{r}
library(ggplot2)
library(GGally)
df <- read.table("balaji-transformed.txt", header=TRUE)
# Histograms and density for each variable
par(mfrow=c(2,3))
for (col in colnames(df)) {
  hist(df[[col]], main=paste("Histogram of", col), xlab=col, border="grey", col="skyblue", probability=TRUE)
  lines(density(df[[col]], na.rm=TRUE), col="red", lwd=2)
}
# Pairplot for correlations
ggpairs(df)

```

```{r}
hist(my.data[, "Y"], main = "Y", xlab = "Y")
```

# minmax normalisation and z-score standardisation and scaling to unit interval

```{r}
# minmax normalisation
minmax <- function(x){
  (x - min(x))/(max(x)-min(x))
}

# z-score standardisation and scaling to unit interval
unit.z <- function(x){
  0.15*((x-mean(x))/sd(x)) + 0.5
}
```

#If all observations are positive, a Box–Cox transform estimates the
“best” power:

```{r}
library(MASS)

boxcox_transform <- function(x, lambda_seq = seq(-2, 2, by = 0.1), plot = FALSE) {
  # x: numeric vector (positive values for Box–Cox)
  # lambda_seq: grid of lambda values to search over
  # plot: whether to show the boxcox profile plot
  
  # Fit Box–Cox over an intercept-only model
  bc <- boxcox(x ~ 1, lambda = lambda_seq, plotit = plot)
  
  # Choose lambda that maximizes log-likelihood
  lambda_hat <- bc$x[which.max(bc$y)]
  
  # Apply the transformation
  if (abs(lambda_hat) < 1e-6) {
    x_bc <- log(x)
  } else {
    x_bc <- (x^lambda_hat - 1) / lambda_hat
  }
  
  # Return both transformed data and chosen lambda
  list(
    x_bc  = x_bc,
    lambda = lambda_hat
  )
}

```

```{r . Grid search over with lambda values from -2 to 2 in steps of 0.1 with Shapiro–Wilk test of normalization  }
normality_score_sw <- function(x, n) {
  x_t <- x ^ n
  # Remove NAs just in case
  x_t <- x_t[is.finite(x_t)]
  # Need at least 3 distinct values for a sensible test
  if (length(unique(x_t)) < 3) return(NA_real_)
  shapiro.test(x_t)$p.value
}

find_n_shapiro <- function(x, n_grid = seq(0, 2, by = 0.05)) {
  scores <- sapply(n_grid, function(n) normality_score_sw(x, n))
  # Exclude NA scores
  valid  <- !is.na(scores)
  if (!any(valid)) stop("No valid n in the grid (all transforms constant or invalid).")
  best_i <- which.max(scores[valid])
  list(
    n_opt   = n_grid[valid][best_i],
    p_opt   = scores[valid][best_i],
    grid    = n_grid,
    scores  = scores
  )
}



```

```{r transform variables}
I <- c( 'X1', 'X2', 'X3', 'X4', 'X5','Y')
variables_to_transform <- my.data[,I]
head(variables_to_transform)
variables_to_transform[,"Y"]=minmax(variables_to_transform[,"Y"])+1e-6  #shift to make all values positive
variables_to_transform[,"X1"]=minmax(variables_to_transform[,"X1"])+1e-6
variables_to_transform[,"X2"]=minmax(variables_to_transform[,"X2"])+1e-6
variables_to_transform[,"X3"]=minmax(variables_to_transform[,"X3"])+1e-6
variables_to_transform[,"X4"]=minmax(variables_to_transform[,"X4"])+1e-6
variables_to_transform[,"X5"]=minmax(variables_to_transform[,"X5"])+1e-6
head(variables_to_transform)
#find optimal n for each variable and transform
res_swY=find_n_shapiro(variables_to_transform[,"Y"])
variables_to_transform[,"Y"]=variables_to_transform[,"Y"]^res_swY$n_opt
res_swX1=find_n_shapiro(variables_to_transform[,"X1"])
variables_to_transform[,"X1"]=variables_to_transform[,"X1"]^res_swX1$n_opt
res_swX2=find_n_shapiro(variables_to_transform[,"X2"])
variables_to_transform[,"X2"]=variables_to_transform[,"X2"]^res_swX2$n_opt
res_swX3=find_n_shapiro(variables_to_transform[,"X3"])
variables_to_transform[,"X3"]=variables_to_transform[,"X3"]^res_swX3$n_opt
res_swX4=find_n_shapiro(variables_to_transform[,"X4"])
variables_to_transform[,"X4"]=variables_to_transform[,"X4"]^res_swX4$n_opt
res_swX5=find_n_shapiro(variables_to_transform[,"X5"])
variables_to_transform[,"X5"]=variables_to_transform[,"X5"]^res_swX5$n_opt
head(variables_to_transform)

```

#plot

```{r}
hist(variables_to_transform[, "Y"], main = "Y", xlab = "Y")
```

```{r}
for (k in paste0("X", 1:5)) { hist(variables_to_transform[, k], main = k, xlab = k) }
```

#save transformed data

```{r}
data.transformed=variables_to_transform;
write.table(data.transformed, "balaji-transformed.txt")  
```

```{r}
source("AggWaFit718.R")
```

########################################## 

#T3 - Build models and investigate
\##########################################

```{r}
source("AggWaFit718.R")
```

```{r}
data.transformed_copy <- data.transformed
  #as.matrix(read.table("balaji-transformed.txt"))
```

# Get weights for Weighted Arithmetic Mean with fit.QAM()

```{r}
fit.QAM(data.transformed_copy,output.1="WAM3.txt",stats.1 = "WAM-stats3.txt")
```

```{r}
data.transformed_copy[1,1]
```

```{r}
length(data.transformed[,6])
  
```

# Get weights for Power Mean p=0.5 and p=2 with fit.QAM()

```{r}
fit.QAM(data.transformed_copy,output.1="powermean3.txt",stats.1 = "powermean-stats3.txt",g=PM05,g.inv=invPM05)
fit.QAM(data.transformed_copy,output.1="powermean32.txt",stats.1 = "powermean-stats32.txt",g=QM,g.inv=invQM)
fit.QAM(data.transformed_copy,output.1="powermean3-1.txt",stats.1 = "powermean-stats3-1.txt",g=HM,g.inv=invHM)
fit.QAM(data.transformed_copy,output.1="powermean30.txt",stats.1 = "powermean-stats30.txt",g=GM,g.inv=invGM)
```

# Get weights for Ordered Weighted Average with fit.OWA()

```{r}
fit.OWA(data.transformed_copy,output.1="OWA3.txt",stats.1 = "OWA-stats3.txt")
```

# Get weights for Choquet Integral with fit.choquet() - Optional

```{r}
fit.choquet(data.transformed_copy,output.1="choquet3.txt",stats.1 = "choquet-stats3.txt")
```

####################################### 

#T4 - Use Model for Prediction \#######################################

```{r imort new data}
# Your data
new.data <- c(23.5, 35.87125, 24.89, 32.93, 758.55)

# Create a matrix with 1 row and 5 columns
new.data <- matrix(new.data, nrow = 1)

# Assign column names
colnames(new.data) <- c("X1", "X2", "X3", "X4", "X5")

# Check result
new.data

```

```{r transform new data}
new_input_to_transform <- new.data
new_input_to_transform[,"X1"]=(new_input_to_transform[,"X1"]-min(my.data[,"X1"]))/(max(my.data[,"X1"])-min(my.data[,"X1"]))+1e-6
new_input_to_transform[,"X2"]=(new_input_to_transform[,"X2"]-min(my.data[,"X2"]))/(max(my.data[,"X2"])-min(my.data[,"X2"]))+1e-6
new_input_to_transform[,"X3"]=(new_input_to_transform[,"X3"]-min(my.data[,"X3"]))/(max(my.data[,"X3"])-min(my.data[,"X3"]))+1e-6
new_input_to_transform[,"X4"]=(new_input_to_transform[,"X4"]-min(my.data[,"X4"]))/(max(my.data[,"X4"])-min(my.data[,"X4"]))+1e-6
new_input_to_transform[,"X5"]=(new_input_to_transform[,"X5"]-min(my.data[,"X5"]))/(max(my.data[,"X5"])-min(my.data[,"X5"]))+1e-6
new_input_to_transform[,"X1"]=new_input_to_transform[,"X1"]^res_swX1$n_opt
new_input_to_transform[,"X2"]=new_input_to_transform[,"X2"]^res_swX2$n_opt
new_input_to_transform[,"X3"]=new_input_to_transform[,"X3"]^res_swX3$n_opt
new_input_to_transform[,"X4"]=new_input_to_transform[,"X4"]^res_swX4$n_opt
new_input_to_transform[,"X5"]=new_input_to_transform[,"X5"]^res_swX5$n_opt
new_input_to_transform
```

```{r predict Y using QM model}
# Load the weights from the best model (QM in this case
weights_QM <- c(0.934,0,0.066,0,0)
# call the function PM
result=PM(new_input_to_transform,weights_QM,-1)
  #QAM(  x = new_input_to_transform,  w = weights_QM,  g = QM,  g.inv = invQM)



```

```{r reverse transform Y to original scale}
result_scaled=result^(1/res_swY$n_opt)
result_scaled
# Reverse min-max scaling for Y
original_min_Y <- min(my.data[,"Y"])
original_max_Y <- max(my.data[,"Y"])
result_original_scale <- result_scaled * (original_max_Y - original_min_Y) + original_min_Y
result_original_scale
```
